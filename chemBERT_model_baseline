{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bddb9b41",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-19T08:57:53.551490Z",
     "iopub.status.busy": "2025-06-19T08:57:53.551218Z",
     "iopub.status.idle": "2025-06-19T09:11:39.650551Z",
     "shell.execute_reply": "2025-06-19T09:11:39.649777Z"
    },
    "papermill": {
     "duration": 826.105186,
     "end_time": "2025-06-19T09:11:39.653539",
     "exception": false,
     "start_time": "2025-06-19T08:57:53.548353",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 08:58:14.379462: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750323494.556744      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750323494.606705      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Validation MAE: nan\n",
      "Epoch 2 Validation MAE: nan\n",
      "Epoch 3 Validation MAE: nan\n"
     ]
    }
   ],
   "source": [
    "# Offline‐capable Polymer Baseline Notebook\n",
    "# -------------------------------------\n",
    "# 0. Pre‑download the model (from a kaggle model)\n",
    "# -------------------------------------------------\n",
    "\n",
    "import os\n",
    "# --- FORCE OFFLINE MODE ---\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "os.environ['HF_DATASETS_OFFLINE'] = '1'\n",
    "os.environ['HF_METRICS_OFFLINE'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# 1. Load Data\n",
    "data_root = '/kaggle/input/neurips-open-polymer-prediction-2025/'\n",
    "train_df = pd.read_csv(os.path.join(data_root, 'train.csv'))\n",
    "test_df = pd.read_csv(os.path.join(data_root, 'test.csv'))\n",
    "\n",
    "# 2. Pre-trained SMILES tokenizer & model (ChemBERTa)\n",
    "\n",
    "#  2. Local SMILES MODEL Path\n",
    "LOCAL_MODEL_DIR = '/kaggle/input/chemberta-zinc-base-v1/transformers/seyonec-chemberta-zinc-base-v1/1/models/ChemBERTa-zinc-base-v1'\n",
    "# Use local_files_only=True to prevent any internet calls\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_DIR, local_files_only=True)\n",
    "base_model = AutoModel.from_pretrained(LOCAL_MODEL_DIR, local_files_only=True)\n",
    "\n",
    "\n",
    "\n",
    "# 3. Dataset Definition\n",
    "class PolymerDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, targets=None, max_length=512):\n",
    "        self.smiles = df['SMILES'].tolist()\n",
    "        self.ids = df['id'].tolist()\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.smiles[idx]\n",
    "        enc = self.tokenizer(s, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        item = {\n",
    "            'input_ids': enc['input_ids'].squeeze(0),\n",
    "            'attention_mask': enc['attention_mask'].squeeze(0)\n",
    "        }\n",
    "        if self.targets is not None:\n",
    "            item['labels'] = torch.tensor(self.targets[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "# 4. Regression Model\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, base_model, num_targets=5):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.regressor = nn.Linear(self.base.config.hidden_size, num_targets)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.base(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = out.last_hidden_state[:, 0, :]  # CLS token pooling\n",
    "        x = self.dropout(pooled)\n",
    "        return self.regressor(x)\n",
    "\n",
    "# 5. Prepare DataLoaders\n",
    "TARGET_COLS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "train_data, val_data = train_test_split(train_df, test_size=0.1, random_state=42)\n",
    "train_ds = PolymerDataset(train_data, tokenizer, targets=train_data[TARGET_COLS].values)\n",
    "val_ds = PolymerDataset(val_data, tokenizer, targets=val_data[TARGET_COLS].values)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=8)\n",
    "\n",
    "# 6. Training Loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RegressionModel(base_model).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.L1Loss()  # MAE\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = {k: batch[k].to(device) for k in ('input_ids', 'attention_mask')}\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs, batch['labels'].to(device))\n",
    "        loss.backward(); optimizer.step()\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = {k: batch[k].to(device) for k in ('input_ids', 'attention_mask')}\n",
    "            outputs = model(**inputs).cpu().numpy()\n",
    "            preds.append(outputs)\n",
    "            trues.append(batch['labels'].numpy())\n",
    "    preds = np.vstack(preds); trues = np.vstack(trues)\n",
    "    val_mae = np.mean(np.abs(preds - trues))\n",
    "    print(f\"Epoch {epoch+1} Validation MAE: {val_mae:.4f}\")\n",
    "\n",
    "# 7. Inference on Test Set\n",
    "\n",
    "test_ds = PolymerDataset(test_df, tokenizer)\n",
    "test_loader = DataLoader(test_ds, batch_size=8)\n",
    "model.eval()\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs = {k: batch[k].to(device) for k in ('input_ids', 'attention_mask')}\n",
    "        outputs = model(**inputs).cpu().numpy()\n",
    "        all_preds.append(outputs)\n",
    "all_preds = np.vstack(all_preds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9186c4f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T09:11:39.658370Z",
     "iopub.status.busy": "2025-06-19T09:11:39.657427Z",
     "iopub.status.idle": "2025-06-19T09:11:39.668860Z",
     "shell.execute_reply": "2025-06-19T09:11:39.668218Z"
    },
    "papermill": {
     "duration": 0.014478,
     "end_time": "2025-06-19T09:11:39.669916",
     "exception": false,
     "start_time": "2025-06-19T09:11:39.655438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Submission saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 8. Create Submission\n",
    "submission = pd.DataFrame(all_preds, columns=TARGET_COLS)\n",
    "submission['id'] = test_df['id']\n",
    "submission = submission[['id'] + TARGET_COLS]\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print('Done. Submission saved to submission.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12609125,
     "isSourceIdPinned": false,
     "sourceId": 74608,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 380157,
     "modelInstanceId": 358847,
     "sourceId": 441303,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 833.661104,
   "end_time": "2025-06-19T09:11:42.890370",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-19T08:57:49.229266",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
