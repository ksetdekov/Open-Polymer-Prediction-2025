{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Offline‐capable Polymer Baseline Notebook\n# -------------------------------------\n# 0. Pre‑download the model (on a connected machine)\n# -------------------------------------------------\n# Install the HF Hub client:\n#   pip install huggingface_hub\n# Use the Hub API to snapshot the ChemBERTa repo locally:\n#   from huggingface_hub import snapshot_download\n#   snapshot_download(\n#       repo_id='seyonec/ChemBERTa-zinc-base-v1',\n#       local_dir='./models/ChemBERTa-zinc-base-v1'\n#   )\n# Then transfer the './models/ChemBERTa-zinc-base-v1' folder to your offline environment.\n\nimport os\n# --- FORCE OFFLINE MODE ---\nos.environ['TRANSFORMERS_OFFLINE'] = '1'\nos.environ['HF_DATASETS_OFFLINE'] = '1'\nos.environ['HF_METRICS_OFFLINE'] = '1'\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel\n\n# 1. Load Data\ndata_root = 'kaggle/input/neurips-open-polymer-prediction-2025'\ntrain_df = pd.read_csv(os.path.join(data_root, 'train.csv'))\ntest_df = pd.read_csv(os.path.join(data_root, 'test.csv'))\n\n# 2. Pre-trained SMILES tokenizer & model (ChemBERTa)\n\n#  2. Local SMILES MODEL Path\nLOCAL_MODEL_DIR = './models/ChemBERTa-zinc-base-v1'\n# Use local_files_only=True to prevent any internet calls\ntokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_DIR, local_files_only=True)\nbase_model = AutoModel.from_pretrained(LOCAL_MODEL_DIR, local_files_only=True)\n\n\n\n# 3. Dataset Definition\nclass PolymerDataset(Dataset):\n    def __init__(self, df, tokenizer, targets=None, max_length=512):\n        self.smiles = df['SMILES'].tolist()\n        self.ids = df['id'].tolist()\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.smiles)\n\n    def __getitem__(self, idx):\n        s = self.smiles[idx]\n        enc = self.tokenizer(s, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n        item = {\n            'input_ids': enc['input_ids'].squeeze(0),\n            'attention_mask': enc['attention_mask'].squeeze(0)\n        }\n        if self.targets is not None:\n            item['labels'] = torch.tensor(self.targets[idx], dtype=torch.float)\n        return item\n\n# 4. Regression Model\nclass RegressionModel(nn.Module):\n    def __init__(self, base_model, num_targets=5):\n        super().__init__()\n        self.base = base_model\n        self.dropout = nn.Dropout(0.1)\n        self.regressor = nn.Linear(self.base.config.hidden_size, num_targets)\n\n    def forward(self, input_ids, attention_mask):\n        out = self.base(input_ids=input_ids, attention_mask=attention_mask)\n        pooled = out.last_hidden_state[:, 0, :]  # CLS token pooling\n        x = self.dropout(pooled)\n        return self.regressor(x)\n\n# 5. Prepare DataLoaders\nTARGET_COLS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\ntrain_data, val_data = train_test_split(train_df, test_size=0.1, random_state=42)\ntrain_ds = PolymerDataset(train_data, tokenizer, targets=train_data[TARGET_COLS].values)\nval_ds = PolymerDataset(val_data, tokenizer, targets=val_data[TARGET_COLS].values)\n\ntrain_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=8)\n\n# 6. Training Loop\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = RegressionModel(base_model).to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\ncriterion = nn.L1Loss()  # MAE\n\nepochs = 3\nfor epoch in range(epochs):\n    model.train()\n    for batch in train_loader:\n        optimizer.zero_grad()\n        inputs = {k: batch[k].to(device) for k in ('input_ids', 'attention_mask')}\n        outputs = model(**inputs)\n        loss = criterion(outputs, batch['labels'].to(device))\n        loss.backward(); optimizer.step()\n    # Validation\n    model.eval()\n    preds, trues = [], []\n    with torch.no_grad():\n        for batch in val_loader:\n            inputs = {k: batch[k].to(device) for k in ('input_ids', 'attention_mask')}\n            outputs = model(**inputs).cpu().numpy()\n            preds.append(outputs)\n            trues.append(batch['labels'].numpy())\n    preds = np.vstack(preds); trues = np.vstack(trues)\n    val_mae = np.mean(np.abs(preds - trues))\n    print(f\"Epoch {epoch+1} Validation MAE: {val_mae:.4f}\")\n\n# 7. Inference on Test Set\n\ntest_ds = PolymerDataset(test_df, tokenizer)\ntest_loader = DataLoader(test_ds, batch_size=8)\nmodel.eval()\nall_preds = []\nwith torch.no_grad():\n    for batch in test_loader:\n        inputs = {k: batch[k].to(device) for k in ('input_ids', 'attention_mask')}\n        outputs = model(**inputs).cpu().numpy()\n        all_preds.append(outputs)\nall_preds = np.vstack(all_preds)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-19T08:01:02.062571Z","iopub.execute_input":"2025-06-19T08:01:02.062783Z","iopub.status.idle":"2025-06-19T08:14:55.420974Z","shell.execute_reply.started":"2025-06-19T08:01:02.062763Z","shell.execute_reply":"2025-06-19T08:14:55.419814Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/166 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b69a13010d464d9d8b8cec2d309e6f7c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/501 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9dcc8f77ef944eca05cb518fb5016b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/9.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8b6aa7afc8f43eb8fb2052733d756a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/3.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dca2ad4ab7a41c1bbd77587b84b6210"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c386056148347dea2e47958164a92c6"}},"metadata":{}},{"name":"stderr","text":"2025-06-19 08:01:27.782372: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750320087.999176      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750320088.066424      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/179M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b49647c68ce46adb5f27716c89c0824"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/179M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b5b7511b1474da7b5522b7bd8deedc9"}},"metadata":{}},{"name":"stdout","text":"Epoch 1 Validation MAE: nan\nEpoch 2 Validation MAE: nan\nEpoch 3 Validation MAE: nan\nDone. Submission saved to submission.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\n# 8. Create Submission\nsubmission = pd.DataFrame(all_preds, columns=TARGET_COLS)\nsubmission['id'] = test_df['id']\nsubmission = submission[['id'] + TARGET_COLS]\nsubmission.to_csv('submission.csv', index=False)\nprint('Done. Submission saved to submission.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T08:14:55.435757Z","iopub.execute_input":"2025-06-19T08:14:55.435994Z","iopub.status.idle":"2025-06-19T08:14:55.456892Z","shell.execute_reply.started":"2025-06-19T08:14:55.435976Z","shell.execute_reply":"2025-06-19T08:14:55.456071Z"}},"outputs":[{"name":"stdout","text":"Done. Submission saved to submission.csv\n","output_type":"stream"}],"execution_count":3}]}